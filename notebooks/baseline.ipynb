{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device is {device}')\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "from transformers import BertTokenizer,BertForSequenceClassification,AdamW\n",
    "\n",
    "import os,gc,warnings\n",
    "gc.collect()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re,string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import random\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "os.environ['PYTHONHASHSEED'] = str(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb --upgrade\n",
    "\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# wandb_key = user_secrets.get_secret(\"wandb_key\")\n",
    "# wandb.login(wandb_key)\n",
    "# !wandb login $key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'model_name':'bert-base-uncased',\n",
    "    'epochs':4,\n",
    "    'batch_size':32,\n",
    "    'learning_rate':5e-6,\n",
    "    'adamw_lr':6e-6,\n",
    "    'adamw_eps':1e-8,\n",
    "    'exp_name':'for-test'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "    entity='chanmuzi',\n",
    "    project=\"Disaster Tweets\",\n",
    "    group=configs['model_name'],\n",
    "    name=configs['exp_name'],\n",
    "    config=configs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing, Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./Data/train.csv') # check path!\n",
    "test = pd.read_csv('./Data/test.csv')\n",
    "train_len = len(train)\n",
    "\n",
    "all_data = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tag(text):\n",
    "    tag = re.compile(r'@\\S+')\n",
    "    return re.sub(tag,'',text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(url,'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<[^>]+>|\\([^)]+\\)')\n",
    "    return re.sub(html,'',text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    punct = list(string.punctuation)\n",
    "    table = str.maketrans('','',''.join(punct))\n",
    "    return text.translate(table)\n",
    "\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['cleaned'] = all_data['text'].apply(lambda x:remove_tag(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x:remove_URL(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x:remove_html(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x:remove_punct(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x:x.lower())\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x:word_tokenize(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x:' '.join([word for word in x if word not in stop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = all_data[:train_len],all_data[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,label):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        text = self.df.loc[idx]['cleaned']\n",
    "\n",
    "        encoded_dict = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=82,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        if self.label:\n",
    "            labels = self.df.loc[idx]['target']\n",
    "            return {'input_ids':encoded_dict['input_ids'].squeeze(),\n",
    "                    'attention_mask':encoded_dict['attention_mask'].squeeze(),\n",
    "                    'labels':torch.tensor(labels,dtype=torch.long).unsqueeze(dim=0)}\n",
    "        else:\n",
    "            return {'input_ids':encoded_dict['input_ids'].squeeze(),\n",
    "                    'attention_mask':encoded_dict['attention_mask'].squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(configs['model_name'])\n",
    "\n",
    "train_dataset = TweetsDataset(train_data,tokenizer,True)\n",
    "test_dataset = TweetsDataset(test_data,tokenizer,False)\n",
    "\n",
    "train_size = int(len(train_dataset) * 0.8)\n",
    "valid_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset,valid_dataset = random_split(train_dataset,[train_size,valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TweetsDataLoader(train_data,test_data,batch_size):\n",
    "    train_dataset = TweetsDataset(train_data,tokenizer,True)\n",
    "    test_dataset = TweetsDataset(test_data,tokenizer,False)\n",
    "\n",
    "    train_size = int(len(train_dataset) * 0.8)\n",
    "    valid_size = len(train_dataset) - train_size\n",
    "\n",
    "    train_dataset,valid_dataset = random_split(train_dataset,[train_size,valid_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=configs['batch_size'],shuffle=True,pin_memory=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset,batch_size=configs['batch_size'],shuffle=False,pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "\n",
    "    print(f'{len(train_dataset)} train samples')\n",
    "    print(f'{len(valid_dataset)} valid samples')\n",
    "    print(f'{len(test_dataset)} test samples')\n",
    "\n",
    "    return  train_dataloader,valid_dataloader,test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,valid_loader,test_loader = TweetsDataLoader(train_data,test_data,configs['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(configs['model_name'])\n",
    "\n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        output = self.model(input_ids,attention_mask)\n",
    "        logits = output.logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TweetsModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(),lr=configs['adamw_lr'],eps=configs['adamw_eps'])\n",
    "metric = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_valid(model,train_loader,valid_loader,criterion,optimizer,metric,epochs):\n",
    "    wandb.watch(model,criterion,log='all',log_freq=10)\n",
    "\n",
    "    best_model_epoch,valid_loss_values = [],[]\n",
    "    valid_loss_min = [1]\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        gc.collect()\n",
    "        pbar = tqdm(train_loader,desc='Training...')\n",
    "        \n",
    "        train_losses = 0\n",
    "        train_steps = 0\n",
    "        for idx,batch in enumerate(pbar):\n",
    "            train_steps += 1            \n",
    "\n",
    "            train_input_ids = batch['input_ids'].to(device)\n",
    "            train_attention_mask = batch['attention_mask'].to(device)\n",
    "            train_labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_outputs = model(train_input_ids,train_attention_mask)\n",
    "\n",
    "            train_loss = criterion(train_outputs,train_labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            train_losses += train_loss.detach().cpu().numpy().item()\n",
    "            wandb.log({\n",
    "                'epoch':epoch,\n",
    "                'train_loss':train_losses / train_steps\n",
    "            })\n",
    "            pbar.set_postfix({'train_loss':train_losses/train_steps})\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] Train_loss: {train_losses/train_steps}')\n",
    "        pbar.close()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            gc.collect()\n",
    "            pabr = tqdm(valid_loader)\n",
    "\n",
    "            valid_steps = 0\n",
    "            valid_losses = 0\n",
    "            valid_score = 0\n",
    "\n",
    "            y_preds,y_labels = [],[]\n",
    "            for idx,batch in enumerate(pbar):\n",
    "                valid_steps += 1\n",
    "\n",
    "                valid_input_ids = batch['input_ids'].to(device)\n",
    "                valid_attention_mask = batch['attention_mask'].to(device)\n",
    "                valid_labels = batch['labels'].to(device)\n",
    "\n",
    "                valid_outputs = model(valid_input_ids,valid_attention_mask)\n",
    "                valid_preds = torch.argmax(valid_outputs,dim=1)\n",
    "\n",
    "                valid_loss = criterion(valid_outputs,valid_labels)\n",
    "                valid_losses += valid_loss.detach().cpu().numpy().item()\n",
    "\n",
    "                y_preds.extend(valid_preds.detach().cpu().numpy())\n",
    "                y_labels.extend(valid_labels.detach().cpu().numpy())\n",
    "            \n",
    "            wandb.log({\n",
    "                'epoch':epoch,\n",
    "                'valid_loss':valid_losses / valid_steps\n",
    "            })\n",
    "            valid_losses /= valid_steps\n",
    "            valid_score = metric(y_labels,y_preds)\n",
    "            print(f'Epoch [{epoch+1}/{epochs}] Score: {valid_score}')\n",
    "            print(f'Epoch [{epoch+1}/{epochs}] Valid_loss: {valid_losses}')\n",
    "\n",
    "            if valid_losses < min(valid_loss_min):\n",
    "                print('model improved!')\n",
    "                torch.save(model.state_dict(), f'save/epoch:{epoch+1}_model.pt')\n",
    "            else:\n",
    "                print('model \"not\" improved..')\n",
    "            \n",
    "            valid_loss_min.append(valid_losses)\n",
    "            print(f'current valid_loss_min:{min(valid_loss_min)}')\n",
    "\n",
    "        best_model_epoch.append(f'save/bert-base/epoch:{epoch+1}_model.pt')\n",
    "        valid_loss_values.append(valid_losses)\n",
    "        print('='*100)\n",
    "    \n",
    "    print('Train/Valid Completed!')\n",
    "    wandb.finish()\n",
    "    select_bset_model(best_model_epoch,valid_loss_values)\n",
    "    \n",
    "    del model,train_loader,valid_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def select_bset_model(best_model_epoch,valid_loss_values):\n",
    "    best_model = best_model_epoch[np.array(valid_loss_values).argmin()]\n",
    "    os.rename(best_model,best_model.split('.pt')[0] + '_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Before training, files in current directory: {os.listdir(\"save\")}')\n",
    "\n",
    "print('Training and Validation Start!')\n",
    "print('='*100)\n",
    "\n",
    "train_and_valid(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    metric,\n",
    "    configs['epochs']\n",
    ")\n",
    "\n",
    "print(f'After training, files in current directory: {os.listdir()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model,test_loader):\n",
    "    test_preds = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(test_loader)\n",
    "        for idx,batch in enumerate(pbar):\n",
    "            test_input_ids = batch['input_ids'].to(device)\n",
    "            test_attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            output = model(test_input_ids,test_attention_mask)\n",
    "            test_pred = torch.argmax(output,dim=1)\n",
    "            test_preds.append(test_pred.detach().cpu().numpy().item())\n",
    "        \n",
    "        return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir():\n",
    "    if 'best.pt' in filename:\n",
    "        best_pt = filename\n",
    "print(f'BEST model.pt: {best_pt}')\n",
    "check_point = torch.load(best_pt)\n",
    "\n",
    "model = TweetsModel(configs['model_name']).to(device)\n",
    "model.load_state_dict(check_point)\n",
    "\n",
    "predictions = inference(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('./Data/sample_submission.csv')\n",
    "predictions = inference(model,test_loader)\n",
    "sample['target'] = predictions\n",
    "sample.to_csv('submission.csv',index=False,header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fecef7fb14ad1d71e869da5296badaba2a50d2b864b0443ebcc6afac654c29b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
