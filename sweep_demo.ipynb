{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchanmuzi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chanmuzi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/chanmuzi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "import os, gc\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re, string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'name':'sweep-test',\n",
    "    'method':'random',\n",
    "    'metric':{\n",
    "        'name':'valid_loss',\n",
    "        'goal':'minimize'\n",
    "    },\n",
    "    'parameters':{\n",
    "        'learning_rate':{\n",
    "            'min':1e-6,\n",
    "            'max':1e-2\n",
    "        },\n",
    "        'epochs':{\n",
    "            'values':[3,4,5,6,7]\n",
    "        },\n",
    "        'batch_size':{\n",
    "            'values':[2,4,8,16,32]\n",
    "        },\n",
    "        'lr':{\n",
    "            'min':1e-7,\n",
    "            'max':1e-2\n",
    "        },\n",
    "        'eps':{\n",
    "            'min':1e-9,\n",
    "            'max':1e-2\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/train.csv')\n",
    "test = pd.read_csv('Data/test.csv')\n",
    "train_len = len(train) # we should record it to split all_data into train,test again\n",
    "\n",
    "all_data = pd.concat([train,test]) # for preprocessing and tokenizing\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "def remove_tag(text):\n",
    "    tag = re.compile(r'@\\S+')\n",
    "    return tag.sub(r'',text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    # http:... / https:... / www... \n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(url,'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    # < > / ( )\n",
    "    html = re.compile(r'<[^>]+>|\\([^)]+\\)')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    # ['!','\"','$','%','&',\"'\",'(',')','*',\n",
    "    # '+',',','-','.','/',':',';','<','=',\n",
    "    # '>','?','@','[','\\\\',']','^','_','`',\n",
    "    # '{','|','}','~']\n",
    "    punctuations = list(string.punctuation)\n",
    "    table = str.maketrans('', '', ''.join(punctuations))\n",
    "    return text.translate(table)\n",
    "\n",
    "all_data['cleaned'] = all_data['text'].apply(lambda x:remove_tag(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: remove_URL(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: remove_html(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: remove_punct(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: x.lower()) # lowering\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: word_tokenize(x)) # split sentence into words list\n",
    "# exclude stop words and make them a sentence again\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: ' '.join([word for word in x if word not in stop]))\n",
    "\n",
    "train_data,test_data = all_data[:train_len],all_data[train_len:]\n",
    "\n",
    "\n",
    "class TweetsDataset(Dataset):\n",
    "    def __init__(self,df,label,tokenizer):\n",
    "        self.df = df # Pandas.DataFrame\n",
    "        self.label = label # True: train,valid / False: test\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) # number of samples\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        text = self.df.loc[idx]['text'] # extracting text from each row\n",
    "\n",
    "        encoded_dict = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=84, # given to the max_length of tokenized text\n",
    "            return_tensors='pt', # PyTorch\n",
    "            return_attention_mask=True, # We should put it into the model\n",
    "        )\n",
    "        '''\n",
    "        The model BERT has two positional(mandatory) arguments,\n",
    "        \"input_ids\" and \"attention_mask\".\n",
    "\n",
    "        In the process of train/valid, we already have labels(answers),\n",
    "        but in case of test, we don't.\n",
    "        '''\n",
    "        if self.label:\n",
    "            labels = self.df.loc[idx]['target']\n",
    "            # [batch,1,max_len(84)] -> [batch,max_len]\n",
    "            return {'input_ids':encoded_dict['input_ids'].squeeze(),\n",
    "                    'attention_mask':encoded_dict['attention_mask'].squeeze(),\n",
    "                    # Our loss_fn wants it to be a \"long tensor\", so it will be changed\n",
    "                    'labels':torch.tensor(labels,dtype=torch.int).unsqueeze(dim=0)}\n",
    "        else:\n",
    "            # [batch,1,max_len(84)] -> [batch,max_len]\n",
    "            return {'input_ids':encoded_dict['input_ids'].squeeze(),\n",
    "                    'attention_mask':encoded_dict['attention_mask'].squeeze()}\n",
    "\n",
    "model_name = 'bert-base-uncased' # If possible, use \"bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TweetsLoader(train_data,batch_size):\n",
    "    train_data_demo = train_data[:200]\n",
    "    train_dataset_demo = TweetsDataset(train_data_demo,True,tokenizer)\n",
    "    train_demo_size = int(0.8 * len(train_dataset_demo))\n",
    "    valid_demo_size = len(train_dataset_demo) - train_demo_size\n",
    "\n",
    "    train_dataset_demo, valid_dataset_demo = random_split(train_dataset_demo,[train_demo_size,valid_demo_size])\n",
    "    print(f'{len(train_dataset_demo)} train demo samples')\n",
    "    print(f'{len(valid_dataset_demo)} valid demo samples')\n",
    "\n",
    "    train_dataloader_demo = DataLoader(train_dataset_demo,batch_size=batch_size,shuffle=True,pin_memory=True)\n",
    "    valid_dataloader_demo = DataLoader(valid_dataset_demo,batch_size=batch_size,shuffle=False,pin_memory=True)\n",
    "    \n",
    "    return train_dataloader_demo,valid_dataloader_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsModel(nn.Module):\n",
    "    def __init__(self,model_name):\n",
    "        super().__init__()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        output = self.model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        logits = output.logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,device,train_dataloader,valid_dataloader,epochs,criterion,optimizer,metric):\n",
    "    wandb.watch(model,criterion,log='all',log_freq=10)\n",
    "\n",
    "    best_model_epoch, valid_loss_values = [],[] \n",
    "    valid_loss_min = [1] # arbitrary loss I set here\n",
    "    for epoch in range(epochs):\n",
    "        gc.collect() # memory cleaning\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        train_step = 0\n",
    "        pbar = tqdm(train_dataloader,desc='Training..')\n",
    "        for idx,batch in enumerate(pbar): # you can also write like \"for batch in tqdm(train_dataloader)\"\n",
    "            optimizer.zero_grad() # initialize\n",
    "            train_step += 1\n",
    "\n",
    "            train_input_ids = batch['input_ids'].to(device)\n",
    "            train_attention_mask = batch['attention_mask'].to(device)\n",
    "            train_labels = batch['labels'].squeeze().to(device).long()\n",
    "            \n",
    "            # You can refer to the class \"TweetsModel\" for understand \n",
    "            # what would be logits\n",
    "            logits = model(train_input_ids, train_attention_mask).to(device)\n",
    "            predictions = torch.argmax(logits, dim=1) # get an index from larger one\n",
    "            detached_predictions = predictions.detach().cpu().numpy()\n",
    "            \n",
    "            loss = criterion(logits, train_labels)\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "            train_loss += loss.detach().cpu().numpy().item()\n",
    "\n",
    "            pbar.set_postfix({'train_loss':train_loss/train_step})\n",
    "            wandb.log({\n",
    "                'epoch':epoch,\n",
    "                'train_loss':train_loss/train_step\n",
    "            })\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] Train_loss: {train_loss/train_step}')\n",
    "        pbar.close()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            valid_loss = 0\n",
    "            valid_step = 0\n",
    "            total_valid_score = 0\n",
    "\n",
    "            y_pred = [] # for getting f1_score that is a metric of the competition\n",
    "            y_true = []\n",
    "\n",
    "            pbar = tqdm(valid_dataloader,desc='Validating...')\n",
    "            for idx,batch in enumerate(pbar):\n",
    "                valid_step += 1\n",
    "\n",
    "                valid_input_ids = batch['input_ids'].to(device)\n",
    "                valid_attention_mask = batch['attention_mask'].to(device)\n",
    "                valid_labels = batch['labels'].squeeze().to(device).long()\n",
    "\n",
    "                logits = model(valid_input_ids, valid_attention_mask).to(device)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                detached_predictions = predictions.detach().cpu().numpy()\n",
    "                \n",
    "                loss = criterion(logits, valid_labels)\n",
    "                valid_loss += loss.detach().cpu().numpy().item()\n",
    "\n",
    "                y_pred.extend(predictions.cpu().numpy())\n",
    "                y_true.extend(valid_labels.cpu().numpy())\n",
    "\n",
    "            wandb.log({\n",
    "                'epoch':epoch,\n",
    "                'valid_loss':valid_loss/valid_step\n",
    "            })                \n",
    "\n",
    "            valid_loss /= valid_step\n",
    "            f1 = f1_score(y_true,y_pred)\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{epochs}] Score: {f1}')\n",
    "            print(f'Epoch [{epoch+1}/{epochs}] Valid_loss: {valid_loss}')\n",
    "\n",
    "            if valid_loss < min(valid_loss_min):\n",
    "                print('model improved!')\n",
    "            else:\n",
    "                print('model not improved')\n",
    "    \n",
    "            # torch.save(model.state_dict(), f'save/epoch:{epoch+1}_model.pt')\n",
    "            # print('save checkpoint!')\n",
    "            valid_loss_min.append(valid_loss)\n",
    "            print(f'valid_loss_min:{min(valid_loss_min)}')\n",
    "        \n",
    "        # # Double check your directory\n",
    "        # best_model_epoch.append(f'save/bert-base/epoch:{epoch+1}_model.pt')\n",
    "        # valid_loss_values.append(valid_loss)\n",
    "        print('='*100)\n",
    "\n",
    "    # select_best_model() # refer to below function\n",
    "    print('Train/Valid Completed!!')\n",
    "    # wandb.finish()\n",
    "    del train_dataloader, valid_dataloader # memory cleaning\n",
    "    gc.collect()\n",
    "\n",
    "# def select_best_model():\n",
    "#     best_model = best_model_epoch[np.array(valid_loss_values).argmin()]\n",
    "#     os.rename(best_model, best_model.split('.pt')[0] + '_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sweep(config=None):\n",
    "    with wandb.init(config=config) as run:\n",
    "        run.name = 'For Test'\n",
    "\n",
    "        w_config = wandb.config\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        train_loader,valid_loader = TweetsLoader(train_data,w_config.batch_size)\n",
    "        model = TweetsModel('bert-base-uncased').to(device)\n",
    "        optimizer = AdamW(model.parameters(),lr=w_config.lr,eps=w_config.eps,no_deprecation_warning=True)\n",
    "        metric = f1_score\n",
    "\n",
    "        train(model,device,train_loader,valid_loader,w_config.epochs,criterion,optimizer,metric)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: bnyj35ht\n",
      "Sweep URL: https://wandb.ai/chanmuzi/sweep_tutorial/sweeps/bnyj35ht\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fsqlv9uy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teps: 0.005864192167837217\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001522159404665369\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.008984468016499101\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/chanmuzi/coding/Kaggle/Competitions/NLP-with-Disaster-Tweets/wandb/run-20230305_161708-fsqlv9uy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chanmuzi/sweep_tutorial/runs/fsqlv9uy' target=\"_blank\">skilled-sweep-1</a></strong> to <a href='https://wandb.ai/chanmuzi/sweep_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/chanmuzi/sweep_tutorial/sweeps/bnyj35ht' target=\"_blank\">https://wandb.ai/chanmuzi/sweep_tutorial/sweeps/bnyj35ht</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chanmuzi/sweep_tutorial' target=\"_blank\">https://wandb.ai/chanmuzi/sweep_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/chanmuzi/sweep_tutorial/sweeps/bnyj35ht' target=\"_blank\">https://wandb.ai/chanmuzi/sweep_tutorial/sweeps/bnyj35ht</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chanmuzi/sweep_tutorial/runs/fsqlv9uy' target=\"_blank\">https://wandb.ai/chanmuzi/sweep_tutorial/runs/fsqlv9uy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 train demo samples\n",
      "40 valid demo samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training..: 100%|██████████| 5/5 [00:19<00:00,  3.83s/it, train_loss=1.05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6] Train_loss: 1.0519946217536926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating...: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6] Score: 0.29629629629629634\n",
      "Epoch [1/6] Valid_loss: 0.6408710479736328\n",
      "model improved!\n",
      "valid_loss_min:0.6408710479736328\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training..: 100%|██████████| 5/5 [00:17<00:00,  3.53s/it, train_loss=0.99] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6] Train_loss: 0.9896473526954651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating...: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6] Score: 0.6666666666666666\n",
      "Epoch [2/6] Valid_loss: 1.0145443975925446\n",
      "model not improved\n",
      "valid_loss_min:0.6408710479736328\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training..: 100%|██████████| 5/5 [00:17<00:00,  3.44s/it, train_loss=0.954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6] Train_loss: 0.9539640545845032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating...: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6] Score: 0.0\n",
      "Epoch [3/6] Valid_loss: 0.7058576047420502\n",
      "model not improved\n",
      "valid_loss_min:0.6408710479736328\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training..: 100%|██████████| 5/5 [00:17<00:00,  3.51s/it, train_loss=0.744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6] Train_loss: 0.7436479926109314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating...: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6] Score: 0.0\n",
      "Epoch [4/6] Valid_loss: 0.7516243159770966\n",
      "model not improved\n",
      "valid_loss_min:0.6408710479736328\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training..: 100%|██████████| 5/5 [00:17<00:00,  3.48s/it, train_loss=0.845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6] Train_loss: 0.8445168495178222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating...: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6] Score: 0.0\n",
      "Epoch [5/6] Valid_loss: 1.031533032655716\n",
      "model not improved\n",
      "valid_loss_min:0.6408710479736328\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training..: 100%|██████████| 5/5 [00:17<00:00,  3.56s/it, train_loss=0.785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6] Train_loss: 0.7846502661705017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating...: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6] Score: 0.0\n",
      "Epoch [6/6] Valid_loss: 0.8860965669155121\n",
      "model not improved\n",
      "valid_loss_min:0.6408710479736328\n",
      "====================================================================================================\n",
      "Train/Valid Completed!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9c484ed10d4d179095823303297a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▄▄▄▄▄▄▅▅▅▅▅▅▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>▃▂▂▆▆▁▁▄▅▅▇█▆▅▅▂▂▃▃▃▃▃▃▃▄▄▃▄▃▃</td></tr><tr><td>valid_loss</td><td>▁█▂▃█▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_loss</td><td>0.78465</td></tr><tr><td>valid_loss</td><td>0.8861</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">skilled-sweep-1</strong> at: <a href='https://wandb.ai/chanmuzi/sweep_tutorial/runs/fsqlv9uy' target=\"_blank\">https://wandb.ai/chanmuzi/sweep_tutorial/runs/fsqlv9uy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230305_161708-fsqlv9uy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7hg91bbg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teps: 0.007926774296926587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.006169208785035573\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.002189506440332318\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/chanmuzi/coding/Kaggle/Competitions/NLP-with-Disaster-Tweets/wandb/run-20230305_161931-7hg91bbg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chanmuzi/sweep_tutorial/runs/7hg91bbg' target=\"_blank\">vocal-sweep-2</a></strong> to <a href='https://wandb.ai/chanmuzi/sweep_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/chanmuzi/sweep_tutorial/sweeps/bnyj35ht' target=\"_blank\">https://wandb.ai/chanmuzi/sweep_tutorial/sweeps/bnyj35ht</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chanmuzi/sweep_tutorial' target=\"_blank\">https://wandb.ai/chanmuzi/sweep_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/chanmuzi/sweep_tutorial/sweeps/bnyj35ht' target=\"_blank\">https://wandb.ai/chanmuzi/sweep_tutorial/sweeps/bnyj35ht</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chanmuzi/sweep_tutorial/runs/7hg91bbg' target=\"_blank\">https://wandb.ai/chanmuzi/sweep_tutorial/runs/7hg91bbg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 train demo samples\n",
      "40 valid demo samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training..: 100%|██████████| 80/80 [00:50<00:00,  1.57it/s, train_loss=0.87] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7] Train_loss: 0.8697851020842791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating...: 100%|██████████| 20/20 [00:02<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7] Score: 0.5964912280701754\n",
      "Epoch [1/7] Valid_loss: 0.7740284085273743\n",
      "model improved!\n",
      "valid_loss_min:0.7740284085273743\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training..: 100%|██████████| 80/80 [00:49<00:00,  1.60it/s, train_loss=0.76] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/7] Train_loss: 0.7598379334434867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating...: 100%|██████████| 20/20 [00:02<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/7] Score: 0.0\n",
      "Epoch [2/7] Valid_loss: 0.6877380043268204\n",
      "model improved!\n",
      "valid_loss_min:0.6877380043268204\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training..: 100%|██████████| 80/80 [00:49<00:00,  1.61it/s, train_loss=0.76] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/7] Train_loss: 0.7595183599740267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating...: 100%|██████████| 20/20 [00:02<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/7] Score: 0.0\n",
      "Epoch [3/7] Valid_loss: 0.6818618655204773\n",
      "model improved!\n",
      "valid_loss_min:0.6818618655204773\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training..:  91%|█████████▏| 73/80 [00:48<00:05,  1.39it/s, train_loss=0.735]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config,project=\"sweep_tutorial\",entity=\"chanmuzi\")\n",
    "wandb.agent(sweep_id,run_sweep,count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fecef7fb14ad1d71e869da5296badaba2a50d2b864b0443ebcc6afac654c29b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
