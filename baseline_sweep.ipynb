{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'{device} is running..')\n",
    "\n",
    "from transformers import BertForSequenceClassification,BertTokenizer,AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import gc,os,random\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "import re,string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'name':'Bert with CE',\n",
    "    'method':'random',\n",
    "    'metric':{\n",
    "        'name':'valid_loss',\n",
    "        'goal':'minimize'\n",
    "    },\n",
    "    'parameters':{\n",
    "        'learning_rate':{\n",
    "            'min':1e-7,\n",
    "            'max':1e-6\n",
    "        },\n",
    "        'epochs':{\n",
    "            'values':[4]\n",
    "        },\n",
    "        'batch_size':{\n",
    "            'values':[16,32]\n",
    "        },\n",
    "        'lr':{\n",
    "            'min':1e-7,\n",
    "            'max':1e-6\n",
    "        },\n",
    "        'eps':{\n",
    "            'min':1e-9,\n",
    "            'max':1e-8\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./Data/train.csv')\n",
    "test_data = pd.read_csv('./Data/test.csv')\n",
    "train_len = len(train_data)\n",
    "\n",
    "all_data = pd.concat([train_data,test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tag(text):\n",
    "    tag = re.compile(r'@\\S+')\n",
    "    return re.sub(tag,'',text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(url,'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<[^>]+>|\\([^)]+\\)')\n",
    "    return re.sub(html,'',text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    punctuation = list(string.punctuation)\n",
    "    table = str.maketrans('','',''.join(punctuation))\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['cleaned'] = all_data['text'].apply(lambda x:remove_tag(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x:remove_URL(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x:remove_punct(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x:x.lower())\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x:word_tokenize(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x:' '.join([word for word in x if word not in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = all_data[:train_len],all_data[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,label):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        text = self.df.loc[idx]['cleaned']\n",
    "\n",
    "        encoded_dict = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=84,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "\n",
    "        if self.label:\n",
    "            labels = self.df.loc[idx]['target']\n",
    "            return {'input_ids':encoded_dict['input_ids'].squeeze(),\n",
    "                    'attention_mask':encoded_dict['attention_mask'].squeeze(),\n",
    "                    'labels':torch.tensor(labels,dtype=torch.long)}\n",
    "        else:\n",
    "            return {'input_ids':encoded_dict['input_ids'].squeeze(),\n",
    "                    'attention_mask':encoded_dict['attention_mask'].squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TweetsLoader(train_data,test_data,batch_size):\n",
    "    train_dataset = TweetsDataset(train_data,tokenizer,True)\n",
    "    test_dataset = TweetsDataset(test_data,tokenizer,False)\n",
    "    train_size = int(len(train_data) * 0.8)\n",
    "    valid_size = len(train_data) - train_size\n",
    "\n",
    "    train_dataset,valid_dataset = random_split(train_dataset,[train_size,valid_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_dataset,batch_size=batch_size,shuffle=False,pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "    return train_loader,valid_loader,test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsModel(nn.Module):\n",
    "    def __init__(self,model_name):\n",
    "        super().__init__()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        output = self.model(input_ids,attention_mask)\n",
    "        return output.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(model,train_loader,valid_loader,criterion,optimizer,metric,epochs):\n",
    "    wandb.watch(model,criterion,log='all',log_freq=10)\n",
    "\n",
    "    valid_loss_list = [1]\n",
    "    for epoch in range(epochs):\n",
    "        gc.collect()\n",
    "        pbar = tqdm(train_loader,desc='Training..')\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_step = 0\n",
    "        for step,batch in enumerate(pbar):\n",
    "            train_step += 1\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids,attention_mask)\n",
    "            loss = criterion(logits,labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "            train_loss += loss.detach().cpu().numpy().item()\n",
    "\n",
    "            pbar.set_postfix({'train_loss':train_loss/train_step})\n",
    "            wandb.log({'train_loss':train_loss/train_step})\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] Train_loss: {train_loss/train_step}')\n",
    "        pbar.close()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            pbar = tqdm(valid_loader,desc='Validating..')\n",
    "\n",
    "            y_pred,y_true = [],[]\n",
    "\n",
    "            valid_loss = 0\n",
    "            valid_step = 0\n",
    "            for step,batch in enumerate(pbar):\n",
    "                valid_step += 1\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                logits = model(input_ids,attention_mask)\n",
    "                predictions = torch.argmax(logits,dim=1)\n",
    "\n",
    "                loss = criterion(logits,labels)\n",
    "                valid_loss += loss.detach().cpu().numpy().item()\n",
    "\n",
    "                y_pred.extend(predictions.detach().cpu().numpy())\n",
    "                y_true.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "            valid_loss /= valid_step\n",
    "            f1 = f1_score(y_true,y_pred)\n",
    "\n",
    "            if valid_loss < min(valid_loss_list):\n",
    "                print('model improved!')\n",
    "            else:\n",
    "                print('model \"not\" improved..')\n",
    "            valid_loss_list.append(valid_loss)\n",
    "\n",
    "            wandb.log({'valid_loss':valid_loss})\n",
    "            wandb.log({'valid_score':f1})\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{epochs}] Score: {f1}')\n",
    "            print(f'Epoch [{epoch+1}/{epochs}] Valid_loss: {valid_loss}')\n",
    "        print('='*100)\n",
    "    print('Train/Valid completed')\n",
    "    \n",
    "    del model,train_loader,valid_loader,criterion,optimizer,metric\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sweep(config=None):\n",
    "    with wandb.init(config=config) as run:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        run.name = 'Bert_base_raw'\n",
    "\n",
    "        w_config = wandb.config\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        train_loader,valid_loader,test_loader = TweetsLoader(train_data,test_data,w_config.batch_size)\n",
    "        model = TweetsModel('bert-base-uncased').to(device)\n",
    "        optimizer = AdamW(model.parameters(),lr=w_config.lr,eps=w_config.eps,no_deprecation_warning=True)\n",
    "        metric = f1_score\n",
    "\n",
    "        train_valid(model,train_loader,valid_loader,criterion,optimizer,metric,w_config.epochs)\n",
    "        del criterion,train_loader,valid_loader,model,optimizer,metric\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config,project='sweep_bert_base',entity='chanmuzi')\n",
    "wandb.agent(sweep_id,run_sweep,count=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fecef7fb14ad1d71e869da5296badaba2a50d2b864b0443ebcc6afac654c29b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
