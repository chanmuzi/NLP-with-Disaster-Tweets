{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data, Preprocessing\n",
    "\n",
    "The dataset we have to feed consist of 'Tweets' according to the kaggle guide,\n",
    "which means that there sould be lots of 'cooloquial expressions','hashtags','links',etc.\n",
    "\n",
    "Thus, we have to \"clean\" this data(I mean, \"text\") before we feed it to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('Data/train.csv')\n",
    "test = pd.read_csv('Data/test.csv')\n",
    "train_len = len(train)\n",
    "\n",
    "all_data = pd.concat([train,test]) # for preprocessing and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Cleaning Functions\n",
    "def remove_tag(text):\n",
    "    tag = re.compile(r'@\\S+')\n",
    "    return tag.sub(r'',text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    # http:... / https:... / www... \n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(url,'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    # < > / ( )\n",
    "    html = re.compile(r'<[^>]+>|\\([^)]+\\)')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    # ['!','\"','$','%','&',\"'\",'(',')','*',\n",
    "    # '+',',','-','.','/',':',';','<','=',\n",
    "    # '>','?','@','[','\\\\',']','^','_','`',\n",
    "    # '{','|','}','~']\n",
    "    punctuations = list(string.punctuation)\n",
    "    table = str.maketrans('', '', ''.join(punctuations))\n",
    "    return text.translate(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spellchecker import SpellChecker\n",
    "\n",
    "# spell = SpellChecker()\n",
    "# def correct_spellings(text):\n",
    "#     corrected_text = []\n",
    "#     misspelled_words = spell.unknown(text.split())\n",
    "#     for word in text.split():\n",
    "#         if word in misspelled_words:\n",
    "#             corrected_text.append(spell.correction(word))\n",
    "#         else:\n",
    "#             corrected_text.append(word)\n",
    "#     return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chanmuzi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/chanmuzi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['cleaned'] = all_data['text'].apply(lambda x:remove_tag(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: remove_URL(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: remove_html(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: remove_punct(x))\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: x.lower()) # lowering\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: word_tokenize(x)) # split sentence into words list\n",
    "# exclude stop words and make them a sentence again\n",
    "all_data['cleaned'] = all_data['cleaned'].apply(lambda x: ' '.join([word for word in x if word not in stop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember we have combined train and test set into one all_data\n",
    "train_data,test_data = all_data[:train_len],all_data[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self,df,is_grad,tokenizer):\n",
    "        self.df = df\n",
    "        self.is_grad = is_grad\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        text = self.df.loc[idx]['text']\n",
    "\n",
    "        encoded_dict = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=84,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        if self.is_grad:\n",
    "            labels = self.df.loc[idx]['target']\n",
    "            return {'input_ids':encoded_dict['input_ids'].squeeze(),\n",
    "                    'attention_mask':encoded_dict['attention_mask'].squeeze(),\n",
    "                    'labels':torch.tensor(labels,dtype=torch.float).unsqueeze(dim=0)}\n",
    "        else:\n",
    "            return {'input_ids':encoded_dict['input_ids'],\n",
    "                    'attention_mask':encoded_dict['attention_mask']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_dataset = TweetDataset(train_data,True,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090 train samples\n",
      "1523 valid samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "valid_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset,valid_dataset = random_split(train_dataset,[train_size,valid_size])\n",
    "\n",
    "print(f'{len(train_dataset)} train samples')\n",
    "print(f'{len(valid_dataset)} valid samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True,pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=32,shuffle=False,pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'model_name':'bert-base-uncased',\n",
    "    'num_labels':2,\n",
    "    'batch_size':32,\n",
    "    'epochs':3,\n",
    "    'learning_rate':5e-6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Never Detach Tensor during forward\n",
    "class TweetsModel(nn.Module):\n",
    "    def __init__(self,model_name):\n",
    "        super().__init__()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        output = self.model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        logits = output.logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU is turning on..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print('GPU is turning on..')\n",
    "else: \n",
    "    device = 'cpu'\n",
    "    print('CPU is turning on..')\n",
    "model = TweetsModel(configs['model_name']).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "# (y_pred,y_label)\n",
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                lr=6e-6,\n",
    "                eps=1e-8,\n",
    "                no_deprecation_warning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric\n",
    "# f1_score(y_label,y_pred)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "metric = f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc,os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "best_model_epoch, valid_loss_values = [],[]\n",
    "\n",
    "def train(model,device,train_dataloader,valid_dataloader,epochs,loss_fn,optimizer,metric):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        gc.collect()\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        train_step = 0\n",
    "        pbar = tqdm(train_dataloader)\n",
    "\n",
    "        for batch in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            train_step += 1\n",
    "\n",
    "            train_input_ids = batch['input_ids'].to(device)\n",
    "            train_attention_mask = batch['attention_mask'].to(device)\n",
    "            train_labels = batch['labels'].squeeze().to(device).long()\n",
    "            \n",
    "            logits = model(train_input_ids, train_attention_mask).to(device)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            detached_predictions = predictions.detach().cpu().numpy()\n",
    "            \n",
    "            loss = loss_fn(logits, train_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "            train_loss += loss.detach().cpu().numpy().item()\n",
    "\n",
    "            pbar.set_postfix({'train_loss':train_loss/train_step})\n",
    "        pbar.close()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            valid_loss = 0\n",
    "            valid_step = 0\n",
    "            total_valid_score = 0\n",
    "            valid_loss_min = [1]\n",
    "\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "\n",
    "            pbar = tqdm(valid_dataloader)\n",
    "            for batch in pbar:\n",
    "                valid_step += 1\n",
    "\n",
    "                valid_input_ids = batch['input_ids'].to(device)\n",
    "                valid_attention_mask = batch['attention_mask'].to(device)\n",
    "                valid_labels = batch['labels'].squeeze().to(device).long()\n",
    "\n",
    "                logits = model(valid_input_ids, valid_attention_mask).to(device)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                detached_predictions = predictions.detach().cpu().numpy()\n",
    "                \n",
    "                loss = loss_fn(logits, valid_labels)\n",
    "                valid_loss += loss.detach().cpu().numpy().item()\n",
    "\n",
    "                y_pred.extend(predictions.cpu().numpy())\n",
    "                y_true.extend(valid_labels.cpu().numpy())\n",
    "\n",
    "            valid_loss /= valid_step\n",
    "            f1 = f1_score(y_true,y_pred)\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{epochs}] Score: {f1}')\n",
    "            print(f'Epoch [{epoch+1}/{epochs}] Valid_loss: {valid_loss}')\n",
    "\n",
    "            if valid_loss < min(valid_loss_min):\n",
    "                print('model improved!')\n",
    "            else:\n",
    "                print('model not improved')\n",
    "    \n",
    "            torch.save(model.state_dict(), f'epoch:{epoch+1}_model.pt')\n",
    "            print('save checkpoint!')\n",
    "            valid_loss_min.append(valid_loss)\n",
    "            print(f'valid_loss_min:{min(valid_loss_min)}')\n",
    "\n",
    "        best_model_epoch.append(f'save/bert-base/epoch:{epoch+1}_model.pt')\n",
    "        valid_loss_values.append(valid_loss)\n",
    "\n",
    "    select_best_model()\n",
    "    print('Train/Valid Completed!!')\n",
    "    del train_dataloader, valid_dataloader\n",
    "    gc.collect()\n",
    "\n",
    "def select_best_model():\n",
    "    best_model = best_model_epoch[np.array(valid_loss_values).argmin()]\n",
    "    os.rename(best_model, best_model.split('.pt')[0] + '_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU is turning on...\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('GPU is turning on...')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('CPU is turning on...')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Start!')\n",
    "print('=' * 100)\n",
    "\n",
    "train(model,\n",
    "    device,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    configs['epochs'],\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    metric)\n",
    "\n",
    "del model, train_dataloader, valid_dataloader\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fecef7fb14ad1d71e869da5296badaba2a50d2b864b0443ebcc6afac654c29b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
